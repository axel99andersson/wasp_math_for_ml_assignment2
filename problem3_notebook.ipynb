{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a12f11",
   "metadata": {},
   "source": [
    "## Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dc6c1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09301c1c",
   "metadata": {},
   "source": [
    "#### Attribute Information of Sklearn Diabetes Dataset\n",
    "The Sklearn Diabetes Dataset include following attributes:\n",
    "\n",
    "- age: Age in years\n",
    "- sex: Gender of the patient\n",
    "- bmi: Body mass index\n",
    "- bp: Average blood pressure\n",
    "- s1: Total serum cholesterol (tc)\n",
    "- s2: Low-density lipoproteins (ldl)\n",
    "- s3: High-density lipoproteins (hdl)\n",
    "- s4: Total cholesterol / HDL (tch)\n",
    "- s5: Possibly log of serum triglycerides level (ltg)\n",
    "- s6: Blood sugar level (glu)\n",
    "\n",
    "Target: Column 11 represents a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "Dataset info from https://www.geeksforgeeks.org/machine-learning/sklearn-diabetes-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5539e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes_sklearn = load_diabetes(as_frame=True)\n",
    "X, y = diabetes_sklearn['data'], diabetes_sklearn['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6062d749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.511817e-19</td>\n",
       "      <td>1.230790e-17</td>\n",
       "      <td>-2.245564e-16</td>\n",
       "      <td>-4.797570e-17</td>\n",
       "      <td>-1.381499e-17</td>\n",
       "      <td>3.918434e-17</td>\n",
       "      <td>-5.777179e-18</td>\n",
       "      <td>-9.042540e-18</td>\n",
       "      <td>9.293722e-17</td>\n",
       "      <td>1.130318e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.072256e-01</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-9.027530e-02</td>\n",
       "      <td>-1.123988e-01</td>\n",
       "      <td>-1.267807e-01</td>\n",
       "      <td>-1.156131e-01</td>\n",
       "      <td>-1.023071e-01</td>\n",
       "      <td>-7.639450e-02</td>\n",
       "      <td>-1.260971e-01</td>\n",
       "      <td>-1.377672e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.729927e-02</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-3.422907e-02</td>\n",
       "      <td>-3.665608e-02</td>\n",
       "      <td>-3.424784e-02</td>\n",
       "      <td>-3.035840e-02</td>\n",
       "      <td>-3.511716e-02</td>\n",
       "      <td>-3.949338e-02</td>\n",
       "      <td>-3.324559e-02</td>\n",
       "      <td>-3.317903e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.383060e-03</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-7.283766e-03</td>\n",
       "      <td>-5.670422e-03</td>\n",
       "      <td>-4.320866e-03</td>\n",
       "      <td>-3.819065e-03</td>\n",
       "      <td>-6.584468e-03</td>\n",
       "      <td>-2.592262e-03</td>\n",
       "      <td>-1.947171e-03</td>\n",
       "      <td>-1.077698e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.807591e-02</td>\n",
       "      <td>5.068012e-02</td>\n",
       "      <td>3.124802e-02</td>\n",
       "      <td>3.564379e-02</td>\n",
       "      <td>2.835801e-02</td>\n",
       "      <td>2.984439e-02</td>\n",
       "      <td>2.931150e-02</td>\n",
       "      <td>3.430886e-02</td>\n",
       "      <td>3.243232e-02</td>\n",
       "      <td>2.791705e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.107267e-01</td>\n",
       "      <td>5.068012e-02</td>\n",
       "      <td>1.705552e-01</td>\n",
       "      <td>1.320436e-01</td>\n",
       "      <td>1.539137e-01</td>\n",
       "      <td>1.987880e-01</td>\n",
       "      <td>1.811791e-01</td>\n",
       "      <td>1.852344e-01</td>\n",
       "      <td>1.335973e-01</td>\n",
       "      <td>1.356118e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age           sex           bmi            bp            s1  \\\n",
       "count  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02   \n",
       "mean  -2.511817e-19  1.230790e-17 -2.245564e-16 -4.797570e-17 -1.381499e-17   \n",
       "std    4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02   \n",
       "min   -1.072256e-01 -4.464164e-02 -9.027530e-02 -1.123988e-01 -1.267807e-01   \n",
       "25%   -3.729927e-02 -4.464164e-02 -3.422907e-02 -3.665608e-02 -3.424784e-02   \n",
       "50%    5.383060e-03 -4.464164e-02 -7.283766e-03 -5.670422e-03 -4.320866e-03   \n",
       "75%    3.807591e-02  5.068012e-02  3.124802e-02  3.564379e-02  2.835801e-02   \n",
       "max    1.107267e-01  5.068012e-02  1.705552e-01  1.320436e-01  1.539137e-01   \n",
       "\n",
       "                 s2            s3            s4            s5            s6  \n",
       "count  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  \n",
       "mean   3.918434e-17 -5.777179e-18 -9.042540e-18  9.293722e-17  1.130318e-17  \n",
       "std    4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  \n",
       "min   -1.156131e-01 -1.023071e-01 -7.639450e-02 -1.260971e-01 -1.377672e-01  \n",
       "25%   -3.035840e-02 -3.511716e-02 -3.949338e-02 -3.324559e-02 -3.317903e-02  \n",
       "50%   -3.819065e-03 -6.584468e-03 -2.592262e-03 -1.947171e-03 -1.077698e-03  \n",
       "75%    2.984439e-02  2.931150e-02  3.430886e-02  3.243232e-02  2.791705e-02  \n",
       "max    1.987880e-01  1.811791e-01  1.852344e-01  1.335973e-01  1.356118e-01  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key statistics of the dataset\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4da9861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05068012, -0.04464164])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e76b4ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    True\n",
       "sex    True\n",
       "bmi    True\n",
       "bp     True\n",
       "s1     True\n",
       "s2     True\n",
       "s3     True\n",
       "s4     True\n",
       "s5     True\n",
       "s6     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.notnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0972ef96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    442.000000\n",
       "mean     152.133484\n",
       "std       77.093005\n",
       "min       25.000000\n",
       "25%       87.000000\n",
       "50%      140.500000\n",
       "75%      211.500000\n",
       "max      346.000000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9faf7a8",
   "metadata": {},
   "source": [
    "**Comment:** Seems like we have 9 numerical features and one binary feature (sex). All numerical features are between 0 and 1 and we don't have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5a7dd383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_SIZE = len(X)\n",
    "TEST_SIZE = int(0.2*DATASET_SIZE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=43)\n",
    "y_train, y_test = pd.DataFrame(y_train), pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "44b2fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data (already pretty similar magnitude of features but can be good anyway)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "y_train = y_scaler.fit_transform(y_train).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c0298ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Selection\n",
    "fig = plt.figure()\n",
    "plt.hist(y_train, bins='auto')\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f4fd9",
   "metadata": {},
   "source": [
    "**Comment:** Difficult to draw any conclusions from this distribution due to the small number of samples. It looks a bit right-skewed but could also be a Gaussian, we will look at variants of standard linear regression (not Poisson regression for instance or other link functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "520af401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model       MSE  R-squared       MAE\n",
      "0     Linear Regression  0.495351   0.542070  0.589807\n",
      "1  Ridge (alpha=33.932)  0.494199   0.543135  0.589785\n",
      "2   Lasso (alpha=0.022)  0.498987   0.538708  0.592001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Function to evaluate test metrics (mse, r2 and mae)\n",
    "    \"\"\"    \n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_test = y_scaler.transform(y_test).reshape(-1,)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'MSE': mse,\n",
    "        'R-squared': r2,\n",
    "        'MAE': mae\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "results.append(evaluate_model(\"Linear Regression\", lr, X_test, y_test))\n",
    "\n",
    "# Ridge Regression with hyperparameter tuning\n",
    "ridge_params = {'alpha': np.logspace(-3, 3, 50)}\n",
    "ridge_cv = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "results.append(evaluate_model(f\"Ridge (alpha={ridge_cv.best_params_['alpha']:.3f})\", ridge_cv.best_estimator_, X_test, y_test))\n",
    "\n",
    "# Lasso Regression with hyperparameter tuning\n",
    "lasso_params = {'alpha': np.logspace(-3, 3, 50)}\n",
    "lasso_cv = GridSearchCV(Lasso(max_iter=10000), lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "results.append(evaluate_model(f\"Lasso (alpha={lasso_cv.best_params_['alpha']:.3f})\", lasso_cv.best_estimator_, X_test, y_test))\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "29863b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model       MSE  R-squared       MAE\n",
      "0     Linear Regression  0.495351   0.542070  0.589807\n",
      "1  Ridge (alpha=33.932)  0.494199   0.543135  0.589785\n",
      "2   Lasso (alpha=0.022)  0.498987   0.538708  0.592001\n",
      "3         Random Forest  0.561015   0.481366  0.608523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "rf_cv = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    rf_params,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    ")\n",
    "rf_cv.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate and append results\n",
    "best_rf = rf_cv.best_estimator_\n",
    "results.append(evaluate_model(\"Random Forest\", best_rf, X_test, y_test))\n",
    "\n",
    "# Display updated results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8bebc134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Number of Trees: 100\n",
      "Random Forest - Maximum Depth of a Tree: 5\n",
      "Random Forest - Minimum number of samples to split an internal node: 2\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the best Random Forest estimator from the grid-search\n",
    "print(\"Random Forest - Number of Trees:\", rf_cv.best_estimator_.n_estimators)\n",
    "print(\"Random Forest - Maximum Depth of a Tree:\", rf_cv.best_estimator_.max_depth)\n",
    "print(\"Random Forest - Minimum number of samples to split an internal node:\", rf_cv.best_estimator_.min_samples_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2624030",
   "metadata": {},
   "source": [
    "#### Methodology and Analysis of Regression Task\n",
    "\n",
    "**Methodology**. \n",
    "\n",
    "We began with checking some key characteristics of the dataset and found that it contained nine numerical features (all in similar range) and one binary feature (sex). We could also see that the range of values in the target variable was a lot different from the rest of the features. In the raw data, the target variable was in the order of $\\sim 10^3$ while the rest of the features were in the order of $\\sim 10^{-2}$.\n",
    "\n",
    "We split the data into a training and test set. The test set consisted of 20% of the total number of samples, the size of the dataset was quite small with only 442 data points.\n",
    "\n",
    "On the training set, we fitted a \"StandardScaler\" which scales the dataset by subtracting the mean and dividing by the standard deviation for each feature. Similarly, a StandardScaler was fitted on the training target values. This means that all features and the target variable will have a mean value of 0 and a standard deviation of 1. This can be useful for overall robustness of training. In cases where gradient based optimization techniques are used, the optimization landscape becomes more \"uniform\" which is useful. In this case we use the default solver to fit models in ```sklearn``` which is ```lbfgs``` which uses the Hessian of the loss function.\n",
    "\n",
    "The models that are invesigated are the standard Linear Regression model, Ridge Regression, Lasso Regression and a Random Forest model. For Ridge, Lasso and Random Forest, there are hyper parameters that have to be tuned for imporved performance. The hyperparameters are tuned with Grid Search Cross Validation. This means we specify a grid of parameter values for each hyperparameter that we test. Then, for each combination of hyperparameters, Leave-One-Out-Cross-Validation is employed to get an estimate of some pre-defined metric that we want to optimize for. In this case, this metric is chosen to be the mean squared error. For Ridge Regression, the only hyperparameter is the value of $\\alpha$ which scales the importance of the $l_2$-term in the loss function. For Lasso Regression, the hyperparameter is also often denoted $\\alpha$ and it scales the importance of the $l_1$-term. For the Random Forest, we employ grid search on more hyperparameters, namely; the number of trees in the forest, the maximum depth of each tree and the minimum number of samples required to split an internal node.\n",
    "\n",
    "To evaluate the models, we employ three metrics; mean squared error (MSE), $R^2$ and mean absolute error (MAE) which are all common metrics in regression tasks.\n",
    "\n",
    "**Analysis of Results**\n",
    "\n",
    "We can see that all models had quite similar performance, especially the three variants of regression. In the final cell output above, it can be seen that Ridge Regression have the lowest MSE and MAE and the highest $R^2$ of all models. I did however run all cells above a few times with different random seeds on the train-test-split and Ridge Regression was not consistently better than the other regression variants. Random Forest did however consistently have the higher MSE and MAE and lower $R^2$ compared to the regression models.\n",
    "\n",
    "Overall we can see that none of the models are particularly good, keep in mind that the target values are standard scaled, this means that an MSE of about 0.5 isn't particularly impressive. The $R^2$ of the models are around 0.5 which indicates that around 50% of the variance is explained by the models. With that said, it is a quite hard task. We are predicting the progression of diabetes over a year from only 10 features from different patients, there will be a lot of variance in this data and potentially a lot of unobserved causes (features we don't have).\n",
    "\n",
    "To improve the performance, my best guess is that more preprocessing and some feature engineering based on domain knowledge would improve results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177103c6",
   "metadata": {},
   "source": [
    "## Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072eecdc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "##### Wine recognition dataset\n",
    "------------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    ":Number of Instances: 178\n",
    "\n",
    ":Number of Attributes: 13 numeric, predictive attributes and the class\n",
    "\n",
    ":Attribute Information:\n",
    "- Alcohol\n",
    "- Malic acid\n",
    "- Ash\n",
    "- Alcalinity of ash\n",
    "- Magnesium\n",
    "- Total phenols\n",
    "- Flavanoids\n",
    "- Nonflavanoid phenols\n",
    "- Proanthocyanins\n",
    "- Color intensity\n",
    "- Hue\n",
    "- OD280/OD315 of diluted wines\n",
    "- Proline\n",
    "- class:\n",
    "    - class_0\n",
    "    - class_1\n",
    "    - class_2\n",
    "\n",
    "\n",
    ":Missing Attribute Values: None\n",
    "\n",
    ":Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
    "\n",
    ":Creator: R.A. Fisher\n",
    "\n",
    ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
    "\n",
    ":Date: July, 1988\n",
    "\n",
    "This is a copy of UCI ML Wine recognition datasets.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
    "\n",
    "The data is the results of a chemical analysis of wines grown in the same\n",
    "region in Italy by three different cultivators. There are thirteen different\n",
    "measurements taken for different constituents found in the three types of\n",
    "wine.\n",
    "\n",
    "Original Owners:\n",
    "\n",
    "Forina, M. et al, PARVUS -\n",
    "An Extendible Package for Data Exploration, Classification and Correlation.\n",
    "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
    "Via Brigata Salerno, 16147 Genoa, Italy.\n",
    "\n",
    "Citation:\n",
    "\n",
    "Lichman, M. (2013). UCI Machine Learning Repository\n",
    "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
    "School of Information and Computer Science.\n",
    "\n",
    "\n",
    "(1) S. Aeberhard, D. Coomans and O. de Vel,\n",
    "Comparison of Classifiers in High Dimensional Settings,\n",
    "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\n",
    "Mathematics and Statistics, James Cook University of North Queensland.\n",
    "(Also submitted to Technometrics).\n",
    "\n",
    "The data was used with many others for comparing various\n",
    "classifiers. The classes are separable, though only RDA\n",
    "has achieved 100% correct classification.\n",
    "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\n",
    "(All results using the leave-one-out technique)\n",
    "\n",
    "(2) S. Aeberhard, D. Coomans and O. de Vel,\n",
    "\"THE CLASSIFICATION PERFORMANCE OF RDA\"\n",
    "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\n",
    "Mathematics and Statistics, James Cook University of North Queensland.\n",
    "(Also submitted to Journal of Chemometrics).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fca948",
   "metadata": {},
   "source": [
    "**Comment:** Given a dataset of chemical analysis of wines, the task is to predict from which cultivator the wine comes from. There are 3 classes (cultivators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "458c9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine_sklearn = load_wine(as_frame=True)\n",
    "X, y = wine_sklearn['data'], wine_sklearn['target']\n",
    "class_names = wine_sklearn['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a5257aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "DATASET_SIZE = len(X)\n",
    "TEST_SIZE = int(0.2*DATASET_SIZE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=14)\n",
    "y_train, y_test = y_train.to_numpy(), y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "cdd6357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "69caa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Accuracy: 1.0000\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        13\n",
      "     class_1       1.00      1.00      1.00        16\n",
      "     class_2       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        35\n",
      "   macro avg       1.00      1.00      1.00        35\n",
      "weighted avg       1.00      1.00      1.00        35\n",
      "\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      " [[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0  6]]\n",
      "\n",
      "Random Forest Accuracy: 1.0000\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        13\n",
      "     class_1       1.00      1.00      1.00        16\n",
      "     class_2       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        35\n",
      "   macro avg       1.00      1.00      1.00        35\n",
      "weighted avg       1.00      1.00      1.00        35\n",
      "\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      " [[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0  6]]\n",
      "\n",
      "Summary Comparison:\n",
      "                  Model  Accuracy\n",
      "0  Logistic Regression       1.0\n",
      "1        Random Forest       1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evaluate_classifier(name, model, X_test, y_test):\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(f\"\\n{name} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=class_names))\n",
    "    print(f\"\\n{name} Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return {'Model': name, 'Accuracy': acc}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Logistic Regression with hyperparameter tuning\n",
    "log_params = {'C': np.logspace(-3, 3, 10)}\n",
    "log_cv = GridSearchCV(LogisticRegression(max_iter=10000, solver='liblinear'), \n",
    "                      log_params, cv=5, scoring='accuracy')\n",
    "log_cv.fit(X_train, y_train)\n",
    "results.append(evaluate_classifier(\"Logistic Regression\", log_cv.best_estimator_, X_test, y_test))\n",
    "\n",
    "# Random Forest Classifier with hyperparameter tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 2, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf_cv = GridSearchCV(RandomForestClassifier(random_state=23), rf_params, cv=5, scoring='accuracy')\n",
    "rf_cv.fit(X_train, y_train)\n",
    "results.append(evaluate_classifier(\"Random Forest\", rf_cv.best_estimator_, X_test, y_test))\n",
    "\n",
    "# Display summary\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary Comparison:\\n\", summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7195ca9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "=========== FULL OUTPUT FROM CELL ABOVE ==============\n",
    "\n",
    "Logistic Regression Accuracy: 1.0000\n",
    "\n",
    "Logistic Regression Classification Report:\n",
    "\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "      class_0         1.00      1.00      1.00        13\n",
    "\n",
    "      class_1         1.00      1.00      1.00        16\n",
    "     \n",
    "      class_2         1.00      1.00      1.00         6\n",
    "    \n",
    "      accuracy        -          -        1.00        35\n",
    "   \n",
    "      macro_avg       1.00      1.00      1.00        35\n",
    "   \n",
    "      weighted_avg    1.00      1.00      1.00        35\n",
    "\n",
    "\n",
    "Logistic Regression Confusion Matrix:\n",
    "\n",
    " [[13  0  0]\n",
    "\n",
    " [ 0 16  0]\n",
    "\n",
    " [ 0  0  6]]\n",
    "\n",
    "Random Forest Accuracy: 1.0000\n",
    "\n",
    "Random Forest Classification Report:\n",
    "\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "      class_0         1.00      1.00      1.00        13\n",
    "\n",
    "      class_1         1.00      1.00      1.00        16\n",
    "     \n",
    "      class_2         1.00      1.00      1.00         6\n",
    "\n",
    "    \n",
    "      accuracy        -         -         1.00        35\n",
    "      macro avg       1.00      1.00      1.00        35\n",
    "      weighted avg    1.00      1.00      1.00        35\n",
    "\n",
    "\n",
    "Random Forest Confusion Matrix:\n",
    "\n",
    " [[13  0  0]\n",
    "\n",
    " [ 0 16  0]\n",
    "\n",
    " [ 0  0  6]]\n",
    "\n",
    "Summary Comparison:\n",
    "\n",
    "               Model          Accuracy\n",
    "\n",
    "         Logistic Regression       1.0\n",
    "\n",
    "         Random Forest             1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4893dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - C (Inverse Regularizing Parameter of L2-penalty) 0.09999999999999999\n",
      "Random Forest - Number of Trees: 50\n",
      "Random Forest - Maximum Depth of a Tree: None\n",
      "Random Forest - Minimum number of samples to split an internal node: 2\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of Best Estimators\n",
    "print(\"Logistic Regression - C (Inverse Regularizing Parameter of L2-penalty)\", log_cv.best_estimator_.C)\n",
    "print(\"Random Forest - Number of Trees:\", rf_cv.best_estimator_.n_estimators)\n",
    "print(\"Random Forest - Maximum Depth of a Tree:\", rf_cv.best_estimator_.max_depth)\n",
    "print(\"Random Forest - Minimum number of samples to split an internal node:\", rf_cv.best_estimator_.min_samples_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a42707",
   "metadata": {},
   "source": [
    "#### Methodology and Analysis of Classification Task\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "The overall workflow was the same here as in the classification task. I'll highlight the differences in the methodologies.\n",
    "\n",
    "The first obvious difference is that this is a classification task, meaning we are predicting a categorical variable instead of a continuous one. In this particular dataset, there are three classes that correspond to different wine cultivators.\n",
    "\n",
    "The models that we consider in this case are Logistic Regression with an $l_2$-penalty term and a Random Forest for classification. For Logistic Regression, there is one hyperparameter to tune, the importance of the penalty term. In the Random Forest Classifier we are tuning the same hyperparameters as we did in the regression task.\n",
    "\n",
    "The performance metrics that we analyze in classification are a bit different to the regression setting. Here, we look at the accuracy, precision, recall, f1-score and confusion matrix. For this task, we don't really have any reason to optimze for precision or recall since false negatives aren't more important than false positive or vice versa. Therefore, in the cross-validation for hyperparameter tuning, we are simply optimizing for accuracy. \n",
    "\n",
    "**Analysis of Results**\n",
    "\n",
    "The result that stands out from the output of the cell above is that both classifiers obtained a test accuracy of 1.0. This dataset seems to be quite easy to classify which definetely wasn't apparent to me beforehand. I also tried to rerun all the cells (under the \"Classification Task\"-title) with different random seeds on the train-test-split and consistently obtained really high accuracies (often 1.0 for both models). I even tried change the train-test ratio from 0.2 to 0.5 and still got a very high accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84a54d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
